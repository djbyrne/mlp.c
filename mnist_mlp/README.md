# MNIST MLP

This project implements a simple Multi-Layer Perceptron (MLP) neural network in C to classify the MNIST dataset. The MNIST dataset consists of 28x28 pixel grayscale images of handwritten digits (0-9) and their corresponding labels.

---

## Table of Contents

1. [Overview](#introduction-to-mlp)
2. [Initialisation](#backpropagation-explanation)
3. [Forward](#code-structure)
4. [Backward](#setup-and-usage)
5. [Benchmarking](#references)
6. [References](#references)

---

## Overview

A **Multi-Layer Perceptron (MLP)** is a type of feedforward neural network consisting of at least three layers:
- **Input Layer:** 784 neurons (representing the 28x28 pixels of an MNIST image).
- **Hidden Layer:** 128 neurons with a relu activation function.
- **Output Layer:** 10 neurons with a softmax activation function (representing digits 0-9).

## Initialisation

## Forward
In forward propagation, the network computes the output for a given input by passing data through the hidden layer and then the output layer.

For each layer:

- The weighted sum of inputs is calculated.
- An activation function is applied to produce the output.
- The output is passed to the next layer as input.


Hidden Layer Computation:
- Compute the weighted sum of inputs plus biases. W is our weight matrix, b is our bias vector, and x is our input vector. This will give us our activation vector z, of size 128.

```math
z^{(1)} = W^{(1)} x + b^{(1)}
```

- Activation: Apply the relu activation function to each element, taking the max of 0 and the input.
```math
a^{(1)} = \max(0, z^{(1)})
```

Output Layer Computation:
- Compute the weighted sum of inputs plus biases.
```math
z^{(2)} = W^{(2)} a^{(1)} + b^{(2)}
```

- Activation: Apply the softmax function to get probabilities of each class (0-9).
```math
a^{(2)} = \text{softmax}(z^{(2)})
```

## Backward

In order to train the network, we need to compute the gradients of the loss function with respect to the weights and biases. This is done using backpropagation, which involves computing the gradients of the loss with respect to the output of each layer. 

### Loss Function
In this example we are using the cross-entropy loss function. Cross-entropy loss is a loss function commonly used in classification tasks within machine learning. It quantifies the difference between the true probability distribution of the labels and the predicted probability distribution generated by the model.

```math
L = -\sum_{k=1}^{10} y_k \log(a^{(2)}_k)
```

Where:
- L is the loss.
- y_k  is the actual label (one-hot encoded).
- a_k  is the predicted probability for class  k.

What this is doing is summing over all classes and taking the negative log of the predicted probability for the true class. 

We take the negative log of the predicted probability for the true class because the softmax function outputs probabilities between 0 and 1, and the log function is negative for values between 0 and 1. 

This means that the loss will be high when the predicted probability for the correct class is low and low when the predicted probability is high.

### Gradients
